{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel,BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#from pytorch_pretrained_bert import BertModel\n",
    "model=BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True,)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'sample', 'text', '.', '[SEP]', 'the', 'other', 'sentence', '.', 'third']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "text=\"sample text.\"\n",
    "text=\"[CLS] \"+text+\" [SEP]\"\n",
    "text=text+\"the other sentence.\"\n",
    "text=text+\"third\"\n",
    "tokens=tokenizer.tokenize(text)\n",
    "seg_ids=[1]*len(tokens)\n",
    "print(tokens)\n",
    "print(seg_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7099, 3793, 1012, 102, 1996, 2060, 6251, 1012, 2353]\n"
     ]
    }
   ],
   "source": [
    "ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 7099, 3793, 1012,  102, 1996, 2060, 6251, 1012, 2353])\n",
      "tensor([[ 101, 7099, 3793, 1012,  102, 1996, 2060, 6251, 1012, 2353]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_10572\\3788750668.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ids_t=torch.tensor(ids_t).unsqueeze(0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ids_t=torch.tensor(ids)\n",
    "seg_t=torch.tensor(seg_ids)\n",
    "print(ids_t)\n",
    "ids_t=torch.tensor(ids_t).unsqueeze(0)\n",
    "print(ids_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "shapes  torch.Size([1, 10, 768])    torch.Size([1, 768])   \n",
      "1    1    13\n",
      "torch.Size([10, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "1\n",
      "10\n",
      "768\n",
      "tensor([[-0.4880, -0.1663, -0.2095,  ..., -0.3476,  0.4867,  0.4252],\n",
      "        [ 0.2041, -0.3386, -0.1902,  ..., -0.1832,  0.5613, -0.0529],\n",
      "        [ 0.3351, -0.0933,  0.1660,  ..., -0.4334,  0.2555,  0.3301],\n",
      "        ...,\n",
      "        [-0.0088, -0.5130, -0.0548,  ..., -0.0487,  0.0143, -0.1981],\n",
      "        [-0.7077, -0.6725,  0.3904,  ...,  0.4060,  0.4872, -0.1999],\n",
      "        [-0.2843, -0.5059,  0.4111,  ..., -0.3996,  0.7107, -0.6139]])\n",
      "tensor([-4.8800e-01, -1.6625e-01, -2.0947e-01, -2.1812e-01, -3.0280e-01,\n",
      "         2.6438e-02,  2.4399e-01,  4.5131e-01, -3.2714e-01, -2.5757e-01,\n",
      "        -4.9264e-01, -7.0161e-02, -1.4464e-01,  1.3770e-01,  2.3711e-01,\n",
      "         3.0323e-01,  1.2400e-03,  2.9459e-01,  2.6913e-01, -5.6465e-01,\n",
      "        -6.4518e-02,  3.6456e-02, -1.7703e-01, -2.1185e-01,  1.8423e-01,\n",
      "        -2.3581e-01, -6.2788e-02, -1.7443e-01, -5.3198e-02,  1.3469e-01,\n",
      "        -1.8795e-01,  3.0411e-01, -3.3787e-01, -5.5883e-03,  5.3936e-01,\n",
      "        -4.3364e-01,  2.7031e-01,  5.7176e-02,  4.5551e-02, -6.0307e-02,\n",
      "        -2.6727e-01, -7.2534e-02,  3.1181e-01, -3.5788e-02, -8.8784e-02,\n",
      "        -7.2544e-02, -2.8938e+00, -2.4148e-01, -4.7157e-01, -2.3494e-01,\n",
      "         1.9705e-02, -9.7863e-02,  5.2417e-01,  4.2887e-01, -3.0843e-01,\n",
      "         6.2653e-01, -2.6352e-01,  6.4111e-01,  2.4343e-01, -1.6617e-01,\n",
      "         1.1626e-01, -3.9987e-01,  1.1336e-02,  3.2657e-01, -2.2440e-01,\n",
      "        -7.5506e-02, -2.3760e-01,  6.2806e-01, -3.6734e-01,  2.5193e-01,\n",
      "        -7.5366e-01, -2.4096e-01,  7.7308e-01, -1.6355e-01,  6.8328e-02,\n",
      "        -4.3280e-02, -8.1907e-02,  2.1043e-01, -5.2943e-01, -2.2063e-01,\n",
      "        -2.5211e-01,  1.8758e-01,  1.4987e-01, -2.4251e-01,  2.8864e-01,\n",
      "         2.8562e-01, -5.3079e-01, -6.8266e-01,  5.1870e-02,  7.2709e-01,\n",
      "        -2.1505e-01, -1.3314e-01, -9.8693e-02,  4.4102e-01,  3.5829e-02,\n",
      "        -3.7601e-02,  9.9276e-04,  2.1195e-01,  9.8517e-02,  4.0122e-01,\n",
      "        -2.4186e-02, -1.8813e-01,  4.5643e-01, -6.5930e-01, -9.2040e-02,\n",
      "        -2.7479e-01, -4.8334e-01, -1.2881e-01,  3.2173e-02, -2.4707e+00,\n",
      "         4.6420e-01,  1.1540e-01,  9.5315e-03,  2.0251e-01, -1.1626e-01,\n",
      "         4.2522e-01,  7.7067e-02, -5.1779e-04,  1.2585e-01,  1.3848e-01,\n",
      "        -4.0025e-02,  3.7779e-01, -2.4128e-01, -2.8229e-01,  7.2013e-02,\n",
      "         3.3471e-01,  6.1549e-01,  9.1829e-02,  5.0989e-01,  4.9183e-02,\n",
      "         2.3422e-01,  5.7318e-01,  1.1910e-01, -1.4438e-01, -2.4995e-01,\n",
      "         1.2928e-01,  6.0002e-01,  2.7671e-01, -1.5568e-01, -2.5264e-01,\n",
      "        -2.3599e-01, -3.8113e-01, -2.7132e+00, -2.1150e-01,  7.9515e-01,\n",
      "         7.1052e-02, -4.9074e-01,  2.8487e-01,  1.3395e-01,  4.6152e-01,\n",
      "         1.3545e-01, -4.3249e-02,  2.9122e-02,  2.1072e-01, -2.2552e-01,\n",
      "         8.6733e-02,  8.0763e-02,  2.0132e-02,  2.5404e-01,  4.6124e-01,\n",
      "         2.6657e-01,  2.8597e-01,  1.4747e-01,  3.3634e-02,  5.8707e-02,\n",
      "         9.6209e-02,  5.5150e-02, -1.2286e-01, -8.0543e-02, -9.0683e-02,\n",
      "        -1.5403e-01,  4.6144e-02,  4.7726e-01,  7.2843e-02, -1.2476e-01,\n",
      "        -1.1340e-01,  1.8370e-02,  4.4363e-01,  5.8983e-01, -3.1815e-01,\n",
      "        -3.6466e-01,  5.3696e-01,  7.4747e-02, -2.5033e-01, -7.6150e-02,\n",
      "        -2.6045e-01,  5.2404e-01, -1.2301e-01, -2.2502e-01,  3.3728e-01,\n",
      "        -1.8237e-01, -3.1593e-01, -8.2682e-02,  1.4616e-01,  2.4845e-01,\n",
      "         1.0305e-01,  3.9993e-01, -3.9754e-01, -4.2222e-02,  3.3251e-01,\n",
      "         1.1536e-01,  3.7946e-03,  3.7326e-02,  6.4462e-02,  4.6110e-01,\n",
      "         3.8337e+00,  3.0787e-02,  1.2895e-01,  2.4328e-01,  5.1254e-01,\n",
      "        -2.3396e-01,  5.1502e-01, -1.4035e-01,  1.1297e-02,  2.1190e-01,\n",
      "         2.1932e-01,  6.1863e-01, -1.0024e-01, -2.0964e-01, -3.0435e-01,\n",
      "        -6.2327e-02,  3.1662e-01, -3.4555e-02,  2.9057e-01, -5.7979e-01,\n",
      "         2.5855e-01,  1.4449e-01,  6.1423e-01, -4.4189e-02, -1.3800e+00,\n",
      "         3.7080e-04,  1.6908e-01,  3.6321e-03,  5.5662e-01, -2.1765e-01,\n",
      "        -4.0544e-01, -2.9831e-01,  7.3525e-02, -7.7175e-02, -8.4517e-02,\n",
      "         7.6081e-02,  2.7984e-01,  2.0962e-01, -4.6676e-02, -4.3327e-01,\n",
      "         3.0455e-01,  6.8603e-01, -2.8650e-01,  8.7530e-03, -1.1735e-01,\n",
      "         5.9391e-01, -2.7089e-01,  3.0163e-01, -5.3154e-01, -5.5740e-02,\n",
      "         9.4422e-02,  2.2580e-02,  1.6392e-01, -2.2847e-01,  5.3401e-02,\n",
      "        -4.4903e-01, -1.8397e-01,  3.7937e-01, -1.1956e-01, -8.9303e-01,\n",
      "        -1.8768e-01, -3.6660e-02, -3.0686e-01,  2.0637e-02, -3.1745e-01,\n",
      "         1.3358e-01, -4.8285e-01, -2.0044e-01, -3.3927e+00,  1.3645e-01,\n",
      "         2.6985e-02,  5.6571e-01,  8.4197e-02, -2.9391e-01,  1.1066e-01,\n",
      "         1.1459e-01,  6.1632e-02, -1.9694e-01,  6.3477e-01,  5.0603e-02,\n",
      "        -3.6915e-01,  6.3014e-01, -2.5369e-01, -4.6487e-03, -2.6416e-01,\n",
      "        -2.8409e-01, -5.6726e-02, -4.3545e-01,  1.0815e-01,  2.8611e-01,\n",
      "         1.6991e-01,  1.8872e-01,  2.5731e-01, -2.8502e-01, -1.0467e-01,\n",
      "        -4.5607e-01,  2.0376e-01, -3.8382e-01, -1.1647e-01, -6.8857e-01,\n",
      "         4.6343e-02, -2.1540e-01, -4.0869e-02, -3.2558e+00,  4.8293e-02,\n",
      "        -2.7124e-01, -2.3380e-01,  3.1734e-02, -3.4428e-02,  3.6601e-01,\n",
      "        -7.4649e-02, -2.2735e-01,  1.9630e-01, -1.8618e-01, -3.9445e-01,\n",
      "        -2.2380e-01,  4.2120e-01,  2.0732e-01,  2.8858e-01,  6.4323e-02,\n",
      "         3.1567e-01,  3.0409e-01,  1.6632e-01, -1.9551e-01, -8.3809e-02,\n",
      "         2.6064e-01, -3.2676e-01,  1.8285e-01,  6.3780e-01, -4.9254e-01,\n",
      "        -1.2286e-01, -2.0201e-01, -8.6800e-02,  1.8263e-01,  6.4082e-03,\n",
      "         1.7836e-01, -2.6947e-01, -6.4380e-01, -2.1686e-01,  3.8497e-01,\n",
      "         4.7402e-01,  6.7842e-01,  2.1584e-01,  1.9085e-02,  9.1743e-01,\n",
      "        -8.5656e-02,  3.2082e-01,  6.9784e-01,  1.8388e-01,  7.2357e-02,\n",
      "        -3.9243e-01,  5.3562e-03,  4.2204e-01, -3.2343e-01, -4.3828e-02,\n",
      "         9.5171e-01,  2.7213e-02,  5.2565e-01, -5.5471e-02,  2.1142e-01,\n",
      "         8.5116e-02,  3.3303e-01,  1.4144e-02,  6.8569e-01, -3.0937e-01,\n",
      "         1.1360e-01, -3.9804e-01,  4.9711e-01, -6.7580e-01,  4.6212e-01,\n",
      "        -1.0278e-01, -2.1745e-02,  1.9652e-01, -1.2116e-01,  1.6293e-01,\n",
      "        -1.3532e-01, -1.3188e+00,  5.1704e-02, -3.0631e-02, -2.5530e-02,\n",
      "         2.1422e-01,  7.9765e-02, -1.7224e-01, -2.6028e-01,  2.7506e-02,\n",
      "        -5.8025e-01,  6.4888e-01, -4.6496e-02, -1.2384e-01,  3.1218e-01,\n",
      "         3.0796e-02, -6.2583e-01, -3.7230e-01, -3.7814e-01,  2.7162e-01,\n",
      "        -6.6801e-02,  1.7238e-01, -3.0839e-01,  4.9170e-01,  8.0935e-01,\n",
      "        -9.0793e-01,  4.8812e-01, -2.9847e-01,  2.8677e-01, -5.9722e-01,\n",
      "        -3.1227e-01,  2.0978e-03, -3.8638e-01,  5.5403e-02, -5.3637e-01,\n",
      "         9.9662e-02, -2.7388e-01,  8.4153e-02,  1.5400e-02,  8.0743e-02,\n",
      "         1.2206e-02,  3.0541e-01,  8.2907e-01, -7.2080e-02, -4.0798e-01,\n",
      "         3.7604e-01,  1.1314e-01,  3.5743e-01,  3.4588e-01,  3.8574e-02,\n",
      "        -1.9991e-01,  9.4812e-04, -3.5835e-01, -4.5929e-01, -4.5746e-02,\n",
      "        -1.2338e-01, -3.8211e-01, -2.6802e-01, -3.4631e-01, -2.0941e-01,\n",
      "        -3.0282e-01, -6.8020e-01,  3.3971e-01, -6.1059e-01, -2.2251e-01,\n",
      "         2.2307e-01,  6.0829e-01, -2.1143e-02,  5.1838e-01, -3.2978e-01,\n",
      "        -4.9434e-01,  5.1967e-01, -9.1602e-02,  7.0081e-01,  7.5092e-02,\n",
      "        -1.8673e-01, -5.0281e-01,  4.3057e-01,  1.8004e-01, -2.1353e-01,\n",
      "        -2.9674e-01, -4.8582e-01,  2.5008e-02, -3.5504e-01, -1.8628e-01,\n",
      "        -1.4848e-01,  1.7473e-01,  1.5811e-01,  1.2305e-01,  2.2372e-01,\n",
      "        -1.6139e+00,  5.7147e-02,  3.6176e-01,  3.8230e-02,  1.2619e-01,\n",
      "         1.4731e-01, -3.4876e-01,  5.0484e-01, -1.8399e-01, -4.7297e-03,\n",
      "        -4.4449e-01,  1.9248e-01,  3.5367e-01,  1.5622e-01, -2.9255e-02,\n",
      "        -3.8318e-02,  2.4834e-01, -9.5185e-02, -1.0845e-01, -2.5294e-01,\n",
      "        -1.3504e-01,  4.6679e-01,  2.9079e-01,  3.1422e-01, -1.2977e-02,\n",
      "        -7.4050e-02, -2.2893e-01,  4.6741e-01,  8.3289e-02,  2.1004e-01,\n",
      "        -1.9485e-01, -2.3582e-01, -7.3582e-01, -2.4359e-01,  1.5526e-01,\n",
      "         4.2583e-01,  1.7242e-01, -1.1972e-01,  7.2428e-01,  4.0143e-01,\n",
      "         6.0767e-02,  5.6261e-01,  3.2400e-01,  6.1900e-02,  5.2374e-01,\n",
      "         4.0975e-01, -3.1993e-01,  1.2629e-01,  2.5461e-01, -6.7016e-01,\n",
      "        -5.4122e-01, -5.4055e-01, -3.7868e-01, -4.7341e-01,  8.4534e-02,\n",
      "        -1.6160e-01,  4.9316e-02,  2.3888e-01, -4.3171e-01, -1.0225e-01,\n",
      "         1.5539e-01, -4.0775e-01, -2.0350e-01,  1.9329e-01, -2.9015e-01,\n",
      "        -6.0967e-01,  3.4733e-01,  1.6726e-02, -1.3415e-01, -8.9014e-02,\n",
      "         5.7504e-01, -8.4489e-02, -1.2103e-01, -3.0800e-02, -6.7442e-01,\n",
      "         4.1568e-02,  2.6855e-01,  1.5035e-01,  4.7640e-01, -4.2859e-02,\n",
      "         2.2580e-02, -7.8235e-01, -4.4631e-01,  2.7652e-01,  8.4431e-02,\n",
      "         2.0707e-01, -8.7027e-02,  3.4038e-01,  1.1115e-01, -3.8804e-02,\n",
      "        -7.3528e-01, -8.0944e-01,  3.8820e-01,  2.1938e-02, -2.7363e-02,\n",
      "        -6.6398e-02, -1.1325e-01,  1.6575e-01,  1.8777e-03,  7.8174e-02,\n",
      "         1.6478e-02,  2.1817e-01,  6.0006e-01,  4.5592e-01,  1.0947e-01,\n",
      "         6.6174e-01,  5.7894e-01,  3.2022e-01, -2.1126e-01, -4.6036e-01,\n",
      "        -2.0262e-01, -1.7954e-01, -2.0178e-02, -2.9494e-01,  1.0338e-01,\n",
      "        -5.6443e-02,  1.4453e-01, -4.9333e-01,  2.0363e+00,  6.3780e-01,\n",
      "         3.1646e-02,  2.0911e-01,  3.4481e-01, -5.0131e-02,  8.1738e-03,\n",
      "         1.5797e-01, -3.7630e-01,  3.5504e-01, -1.7325e-01,  2.9558e-01,\n",
      "        -2.1436e-01,  2.2803e-01,  2.1310e-01,  3.4836e-01,  1.0566e-01,\n",
      "        -3.0675e-01, -5.6309e-01, -9.8932e-02,  2.1173e-02,  6.0791e-01,\n",
      "         3.0020e-01, -1.0940e-01, -6.0111e-02,  3.1991e-01,  1.1529e-01,\n",
      "        -2.8573e-01,  3.7243e-01,  3.3914e-01,  4.0894e-02, -2.0036e-02,\n",
      "         2.1488e-01,  1.1465e-01, -2.2814e-01,  3.8369e-02, -2.0540e-01,\n",
      "        -2.9932e-01, -8.8687e-02,  1.9313e-02,  4.1967e-01, -3.9829e-01,\n",
      "         5.3547e-01,  6.8725e-02, -4.4052e-01,  6.5312e-01, -2.3354e-01,\n",
      "        -1.2738e-01,  3.7195e-01, -1.0849e-02,  2.7678e-01,  2.8268e-01,\n",
      "        -5.0162e-01, -7.2376e-03, -4.9403e-01, -5.1528e-01,  2.7799e-01,\n",
      "        -1.3135e-01, -1.4226e-01,  2.3842e-01,  1.5121e-01,  1.1534e-01,\n",
      "         2.0506e-01, -7.6649e-01, -2.7784e-01, -5.9004e-02, -2.8821e-01,\n",
      "         9.7496e-02,  1.2625e-01, -1.0366e-01,  6.1547e-02,  3.8815e-01,\n",
      "         1.9762e-01,  3.0691e-01,  1.8471e-01,  1.4115e-01,  1.5914e-02,\n",
      "        -2.4405e-01, -2.1951e-01, -2.6151e+00, -2.4664e-03,  3.0495e-01,\n",
      "         1.4859e-01,  1.0954e-01,  3.9936e-01,  1.8580e-01, -6.7568e-02,\n",
      "         1.7999e-01,  1.5824e-01,  1.7284e-01,  4.0916e-01,  2.1190e-01,\n",
      "         4.2942e-02, -1.6615e-01,  1.9056e-01,  4.5714e-01, -6.1351e-02,\n",
      "         1.8987e-01, -2.0896e-01,  4.2902e-01,  7.1452e-02, -4.9198e-02,\n",
      "        -2.1199e-01, -2.3111e-01, -1.5826e-01, -1.6888e-01, -5.1449e-02,\n",
      "         4.6331e-01,  3.9201e-01, -1.0287e-01,  1.9053e-01, -3.4532e-01,\n",
      "         4.4814e-02, -2.5269e-01, -4.8209e-01,  1.5465e-01,  7.0682e-02,\n",
      "         1.3195e-01,  2.1636e-01, -3.1115e-01,  5.2306e-01, -2.2162e-01,\n",
      "         3.5359e-01, -3.0604e-01,  6.1333e-03,  5.3916e-01, -1.7671e-01,\n",
      "         4.8155e-01, -1.3327e-01, -9.4025e-02,  8.7432e-02,  3.4715e-01,\n",
      "        -1.4478e-03,  4.8984e-01,  1.4878e-01,  3.6871e-01, -1.8972e-01,\n",
      "        -1.9034e-01, -1.7302e-01,  1.3865e-01, -6.9150e-02, -4.3393e-02,\n",
      "         2.5876e-02,  8.0244e-01, -3.4784e-01, -8.4516e-02, -1.2157e-01,\n",
      "         7.8643e-02, -3.4747e-02, -3.5978e-01,  8.3853e-02,  5.0266e-01,\n",
      "         2.8825e-01, -3.0799e-01,  3.1949e-01,  4.5154e-01, -1.3586e-01,\n",
      "         5.8742e-01,  9.7982e-02, -4.9832e-02, -4.2551e-01, -8.8488e-02,\n",
      "         3.2480e-01,  4.5363e-01, -7.4982e+00, -1.9341e-01, -3.4978e-01,\n",
      "        -3.9764e-01,  5.3869e-02, -3.1475e-01,  1.5721e-01,  1.0163e-01,\n",
      "        -1.3850e-01,  2.3124e-01, -1.8243e-01,  1.0200e-01,  7.2704e-02,\n",
      "        -3.4757e-01,  4.8669e-01,  4.2516e-01])\n",
      "tensor(-0.1918)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs=model(ids_t)\n",
    "    embed=(outputs[0])\n",
    "    print(len(outputs))\n",
    "    \n",
    "#print(embed.shape)\n",
    "    h0=outputs[0]\n",
    "    h1=outputs[1]\n",
    "    h2=outputs[2]\n",
    "    print(\"shapes \",h0.shape,\"  \",h1.shape,\"  \")\n",
    "    print(len(h0),\"  \",len(h1),\"  \",len(h2))\n",
    "    #print(len(h0[0]))\n",
    "    #print(len(h1[0]))\n",
    "    print(h0[0].shape)\n",
    "    print(h1[0].shape)\n",
    "    print(h0[0][0].shape)\n",
    "    print(len(h2[0]))\n",
    "    print(len(h2[0][0]))\n",
    "    print(len(h2[0][0][0]))\n",
    "\n",
    "    print(h0[0])  #=h1[0]\n",
    "    print(h0[0][0])\n",
    "    print(h0[0][3][767])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "1\n",
      "10\n",
      "768\n",
      "torch.Size([13, 1, 10, 768])\n",
      "torch.Size([13, 10, 768])\n",
      "torch.Size([10, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "print(len(h2))\n",
    "embedings=torch.stack(h2,dim=0)\n",
    "print(len(embedings[0]))\n",
    "print(len(embedings[0][0]))\n",
    "print(len(embedings[0][0][0]))\n",
    "print(embedings.shape)\n",
    "embedings=torch.squeeze(embedings,dim=1)\n",
    "print(embedings.shape)\n",
    "embedings=embedings.permute(1,0,2)\n",
    "print(embedings.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_10572\\1276239137.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#sentiment analysis using weights of Bert and then Logistic Regression\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  a stirring , funny and finally transporting re...  1\n",
       "1  apparently reassembled from the cutting room f...  0\n",
       "2  they presume their audience wo n't sit still f...  0\n",
       "3  this is a visually stunning rumination on love...  1\n",
       "4  jonathan parker 's bartleby should have been t...  1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df1=df[0:400]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
      "1      [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
      "2      [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
      "3      [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
      "4      [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
      "                             ...                        \n",
      "395    [101, 1996, 2732, 2040, 3271, 2507, 1037, 1212...\n",
      "396    [101, 1037, 2204, 2143, 2007, 1037, 5024, 2187...\n",
      "397    [101, 1037, 28851, 2836, 2011, 10786, 8840, 13...\n",
      "398    [101, 1037, 4030, 3048, 2610, 24508, 10874, 20...\n",
      "399    [101, 2144, 4830, 14227, 2121, 16511, 2000, 31...\n",
      "Name: 0, Length: 400, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tokenized=df1[0].apply((lambda x: tokenizer.encode(x,add_special_tokens=True) ))\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "tensor([[  101,  1037, 18385,  ...,     0,     0,     0],\n",
      "        [  101,  4593,  2128,  ...,     0,     0,     0],\n",
      "        [  101,  2027,  3653,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1037, 28851,  ...,     0,     0,     0],\n",
      "        [  101,  1037,  4030,  ...,     0,     0,     0],\n",
      "        [  101,  2144,  4830,  ...,     0,     0,     0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "print(max_len)\n",
    "padded=np.array([i+[0]*(max_len-len(i))for i in tokenized.values])\n",
    "inp=torch.tensor(padded)\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 54)\n",
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "torch.Size([400, 54])\n"
     ]
    }
   ],
   "source": [
    "attention=np.where(padded!=0,1,0)\n",
    "print(attention.shape)\n",
    "print(attention)\n",
    "print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True,)\n",
    "attention=torch.tensor(attention)\n",
    "with torch.no_grad():\n",
    "    last_hidden=model2(inp,attention_mask=attention)\n",
    "#last=last_hidden[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=last_hidden[0][:,0,:].numpy()\n",
    "labels=df1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features,test_features,train_labels,test_labels=train_test_split(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Git\\Projects\\1\\1\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR=LogisticRegression()\n",
    "LR.fit(train_features,train_labels)\n",
    "LR.score(test_features,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n",
      "0.8001960784313726\n",
      "0.8301886792452831\n",
      "[[38 11]\n",
      " [ 7 44]]\n",
      "(2, 2)\n",
      "38 11 7 44\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as met\n",
    "predict=LR.predict(test_features)\n",
    "print(met.accuracy_score(predict,test_labels))\n",
    "print(met.average_precision_score(predict,test_labels))\n",
    "print(met.f1_score(predict,test_labels))\n",
    "cm=met.confusion_matrix(test_labels,predict)\n",
    "print(cm)\n",
    "print(cm.shape)\n",
    "tn,fp,fn,tp=cm.ravel()\n",
    "print(tn,fp,fn,tp)\n",
    "#End of sentiment analsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
